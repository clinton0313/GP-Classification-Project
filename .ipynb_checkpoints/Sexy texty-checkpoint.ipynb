{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa974b03",
   "metadata": {},
   "source": [
    "# High level shit (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18a5a8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784f0994",
   "metadata": {},
   "source": [
    "## GP\n",
    "\n",
    "Gaussian process (GP) is a stochastich process, a non parametric model which assigns a normal distribution to each point. This in turn returns a multivariate normal (MVN) distribution. The difference between a MVN and a GP is that the dimension of the parameters are not fixed for the GP, the covariance matrix is also measured in a very different way. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X &\\sim \\textrm{MVN}(\\mu, \\Sigma)\\\\\n",
    "X &\\sim \\textrm{GP}(\\mu, \\mathbb{K})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the MVN $\\mu$ and $\\Sigma$ are of fixed dimension. $\\Sigma$ represents the covariance matrix in the usual way (Positive definite). For the GP $\\mu$ and $\\mathbb{K}$ are of variable dimension. $\\mathbb{K}$ represent the kernel matrix where \n",
    "\n",
    "$$\n",
    "\\mathbb{K}_{ij} = k(x_{i},x_{j})\n",
    "$$\n",
    "\n",
    "Instead of the usual covariances we substitute a different similarty measured encapsulated by a kernel function (a different dot product)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be81d5d",
   "metadata": {},
   "source": [
    "## What we did (and why)\n",
    "\n",
    "We want to use a GP on a simple binary classification problem. So our goal would be to fit our data to a GP, $\\{ (X_{1},Y_{1}), \\dots, (X_{n}, Y_{n})\\}$ where $X_{i} \\in \\mathbb{R}^{d}$ and $Y_{i} \\in \\{-1, 1\\}$ and \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f() | \\mathbb{K} &\\sim \\mathrm{GP}(0, \\mathbb{K})\\\\\n",
    "Y_{i} &\\sim Bernoulli(\\sigma(f(X_{i})))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "So to do some predictions we need to sample for the posterior which is: \n",
    "\n",
    "$$\n",
    "P(Y_{n+1} = 1 | X_{n+1}, X_{1:n}, Y_{1:n}) = \\int P(Y_{n+1} = 1 | f(x_{n+1}))P(f(x_{n+1})| X_{1:n}, Y_{1:n})d f(X_{n+1})  \n",
    "$$\n",
    "\n",
    "## Why we need a sampler\n",
    "\n",
    "Which is quite intractable since the product of the prior and the likelihood does not produce a new distribution that we can represent analitically. Why is this a problem? We cant marginalize easily so the integral has a complex structure that we can't evaluate (some integrals are very hard and need numerical methods to approximate). Why can't we just sample from the two distributions that we do have and just do the product of that (like in the EM algorithm)? We can't sample $f()$ for the prior and use that parameter for the likelihood since the dependence occurs simultaneously not in order (the disributions have a strong dependence), we have to sample simultaneously not one by one. So we have to construct a sampler that takes values from the true posterior to recreate the integral.\n",
    "\n",
    "## Elliptical slice sampler\n",
    "\n",
    "The elliptical slice sampler samples $f()$ from the posterior distribution which we use to predict the classification of new, unclassified data.\n",
    "\n",
    "The sampler works by using the property that for fixed data, the GP process is a MVN with the same dimension of the data. $f()| \\mathbb{K} \\sim \\mathrm{MVN}(0, \\mathbb{K})$ so the idea is to find new $f'()$ which correspond to our latent variables in such a way that the new $f´()$ deppends on the prior (MVN of dimension n) and the likelihood function of $Y$ (in our case a Bernoulli likelihood function). $Y_{i}| f'() \\sim \\mathrm{Bernoulli}(\\sigma(f'(X_{i})))$ finding the appropriate $f'()$ so we can evaluate $P(Y_{n+1})| X_{n+1}, X_{1:n}, Y_{1:n})$ properly. \n",
    "\n",
    "We start with an initialization of f() and judge if this is a good fit for our data, the likelihood function serves to measure how good of a fit $f()$ is to the observed $Y_{1:n}$. We look for a new $f´()$ to see if we can find a better fit, so we search for one in the posterior space. We create an ellipse that passes through our last $f()$ and the ellipse shape and direcction come from a the MVN covariance matrix (our prior). We randomly check a point of that elipse and check if that new point (which represents a draw of the latent variables) provides a better likelihood, if it doesnt we shrink and sample again from the shrunken chunk of the ellipse. \n",
    "\n",
    "This way we sample many good representations of our latent variables (one for each of our points) that correspond to high likelihood zones or modes.\n",
    "\n",
    "Note that the likelihood is dependent on the observed data but the kernel matrix is not, meaning that the $f()$ is not necessarily the same size as the set of $Y_{i}$s. Using this fact we can do predictions (which is what we wanted to do in the first place). How does it work?\n",
    "\n",
    "## Prediction\n",
    "\n",
    "\n",
    "Say we separate our data into two types, those of which we have a label of dimension $n \\times d$ for and those of which we want to predict the label for of dimension of $m \\times d$. Since we are using a GP, the covariance matrix is built using both types, so the GP has a kernel matrix of dimension $n+m \\times n+m$. When we run our sampler $f()'$ has a dimension of $n+m \\times 1$ meaning that it has a value for all $X$'s irrespective of their labeling. We can still use the likelihood function since the evaluation only considers those data which have a label, meaning that we can still get a best fit, but just for the labeled data. The idea is that use of a similarity measure (kernel functions) will be appropriate for the non labeled data if these are similar in the kernel sense to the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c4a97",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "We note that the prediction strongly depends on the kernel we choose. The kernel and hyperparameter choice has the ability to impart prior knowledge to the problem at hand. The hyperparameters can be learned using traditional methdos but the kernel selection requieres more nuance. \n",
    "\n",
    "To talk about kernels we need to talk about some properties that will facilitate communication:\n",
    "- The sum of two kernels is a kernel\n",
    "- The product of two kernel is a kernel\n",
    "\n",
    "So when we talk about the kernel function that represents a GP we could be talking about many kernel functions that represent properties of the data. Here we mention the combination that we will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baeccc",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36470f6f",
   "metadata": {},
   "source": [
    "## Gaussian Noise Kernel\n",
    "\n",
    "$$\n",
    "k_{1}(x,x) = \\sigma_{1}^{2}I_{n}\n",
    "$$\n",
    "\n",
    "The white noise kernel represents independent and identically distributed noise added to the GP distribution. For most problem it is a reasonable assumption that noise is inevitable so adding a noise kernel to a GP is justified.\n",
    "\n",
    "##  Exponentiated Quadratic Kernel \n",
    "\n",
    "$$\n",
    "k_{2}(x,y) = \\sigma_{2}^{2} e^{-\\frac{\\norm{x-y}^2}{\\mathcal{l}^{2}}}\n",
    "$$\n",
    "\n",
    "This is a very flexible kernel since it represents the prediction in a smoothed (continuous) way. If the points are similar the quadratic kernel is high and if the points are not so similar the quadratic kernel decays at the speed of a Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddcbd9",
   "metadata": {},
   "source": [
    "We will use the kernel $k(x,y) = k_{1}(x,y) + k_{2}(x,y)$ for our analysis. The flexibility that the exponential kernel allows us to express with the added noise kernel and it is simple and doesn't require assumptions that we are unaware of. If we had more knowledge surrounding hear disease data we could set very reasonalbe hyperparameters. We set $\\sigma_{1} = var(x)$, $\\sigma_{2} = 2$, $l = 2$ as not very informative priors we will also try to fit these parameters using regular methods. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python397jvsc74a57bd0ace80fc9aaa53207f18bbd328efc32ba08bd089069489a8582adea0e919566cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
